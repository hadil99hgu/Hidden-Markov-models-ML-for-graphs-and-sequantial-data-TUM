{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project task 01: Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. tl;dr\n",
    "\n",
    "Complete the functions \n",
    "* `HMM_TxtGenerator.forwards` (2pt)\n",
    "* `HMM_TxtGenerator.log_likelihood` (2pt)\n",
    "* `HMM_TxtGenerator.backwards` (2pt)\n",
    "* `HMM_TxtGenerator.generate_sentence` (2pt)\n",
    "\n",
    "in `generator.py`\n",
    "\n",
    "and\n",
    "* `classify_review` (2pt)\n",
    "\n",
    "in `classification.py`.\n",
    "\n",
    "Then push `generator.py` and `classification.py` to the remote Artemis repository.\n",
    "\n",
    "Only functionality in the python files will be graded. Carefully read the method docstrings to understand the task, parameters and what output is expected.  \n",
    "Please make sure that you do not modify any of the other functions or class attributes, since it may interfere with the automatic grading.\n",
    "\n",
    "If you would like some additional context for what we are implementing, you can go through the notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from scipy.sparse import linalg as spl\n",
    "from collections import defaultdict\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "from generator import HMM_Params, HMM_TxtGenerator\n",
    "from classification import classify_review\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "In this project task, we use hidden markov models (HMMs) as a probabilistic generative model for text data. Intuitively, we can think of each latent variable $Z_t \\in \\lbrace 1,...,K \\rbrace$ as, e.g., a hidden part-of-speech tag (like noun, verb, or adjective). Each observation $X_t\\in \\lbrace 1,...,V \\rbrace$ is a word. $K$ denotes number of possible states and $V$ denotes the number of words in our vocabulary. The model generates a sequence of words (i.e. a sentence) as follows:\n",
    "* Generate $Z_1$ from the initial probability distribution ${\\pi} \\in \\mathbb{R}^K$:\n",
    "\\begin{equation}\n",
    "Pr(Z_1=k) = \\mathbf{\\pi}_k.\n",
    "\\end{equation}\n",
    "* Given $Z_1$, generate $Z_2,Z_3,...,Z_t$ as :\n",
    "\\begin{equation}\n",
    "Pr(Z_{t+1}=j|Z_t=i) = \\mathbf{A}_{ij},\n",
    "\\end{equation}\n",
    "where $\\mathbf{A} \\in \\mathbb{R}^{K\\times K}$ is the state transition probability matrix.\n",
    "\n",
    "* Given $Z_t$, generate $X_t$ as :\n",
    "\\begin{equation}\n",
    "Pr(X_{t}=v|Z_t=i) = \\mathbf{B}_{iv},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will use HMMs to classify restaurant reviews as either 1-star or 5-star. We will do so via the following procedure:\n",
    "* We select a subset of 1-star (resp. 5-star) reviews as a training set. (will be stored in variables `reviews_1star_train` (resp. `reviews_5star_train`))  \n",
    "* We learn **two** HMMs: HMM $\\mathcal{H}_1$ on `reviews_1star_train` and HMM $\\mathcal{H}_5$ on `reviews_5star_train`.\n",
    "* In the test phase, we classify a sentence based on the likelihood of the sentence in $\\mathcal{H}_1$ and $\\mathcal{H}_5$ and the class probability of 1-star and 5-star reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with a subset of reviews for restaurants in Las Vegas. The reviews that we'll be working with are either 1-star or 5-star. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"task_01_data.npy\", allow_pickle=True)\n",
    "reviews_1star = data.item()[\"reviews_1star\"]\n",
    "reviews_5star = data.item()[\"reviews_5star\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `reviews_1star` (resp. `reviews_5star`) is a list of sentences of 1-star (resp. 5-star) reviews. Each sentence is itself a list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect what the sentences look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1-star review: ['This', 'place', 'tops', 'the', 'least', 'favorite', 'list', 'by', 'a', 'long', 'shot']\n",
      "a 5-star review: ['Filet', 'mignon', 'and', 'lobster', 'tail', 'was', 'very', 'good']\n"
     ]
    }
   ],
   "source": [
    "print(\"a 1-star review: \" + str(reviews_1star[1]))\n",
    "print(\"a 5-star review: \" + str(reviews_5star[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "Each sentence is assumed to be generated from an HMM $\\mathcal{H}$ as explained in introduction.\n",
    "We denote the parameters of $\\mathcal{H}$ by $ \\mathbf{A},\\mathbf{B},\\pi$.  \n",
    "We store the model parameters using the `HMM_Params` class in `generator.py`. It contains three randomly-initialized stochastic matrices $\\mathbf{A}\\in \\mathbb{R}^{K\\times K}$, $\\mathbf{B}\\in \\mathbb{R}^{K\\times V}$, and $\\pi \\in \\mathbb{R}^{K\\times 1}$.\n",
    "You do not need to change anything in this class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hidden Markov Model is implemented in the class `HMM_TxtGenerator` in `generator.py`.  \n",
    "**Before proceeding, implement the following functions**, which will be used by the EM algorithm:\n",
    "* `HMM_TxtGenerator.forwards` (2pt)\n",
    "* `HMM_TxtGenerator.log_likelihood` (2pt)\n",
    "* `HMM_TxtGenerator.backwards` (2pt)\n",
    "\n",
    "For parameter descriptions, please refer to the docstrings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `HMM_TxtGenerator` is a model which will be defined only on one corpus. We are going to make two instances of this class, one for 1-star reviews and one for 5-star reviews.  \n",
    "To train our model, we split each set of reviews (i.e. `reviews_1star` and `reviews_5star`) into training/test sets.\n",
    "You can change percentage of train/test instances by setting `train_percentage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 80\n",
    "def split_to_traintest(in_list,percentage):\n",
    "    n_train = math.floor(len(in_list)*percentage/100)\n",
    "    n_test  = len(in_list) - n_train\n",
    "    return in_list[0:n_train],in_list[n_train:]\n",
    "reviews_1star_train , reviews_1star_test = split_to_traintest(reviews_1star,train_percentage)\n",
    "reviews_5star_train , reviews_5star_test = split_to_traintest(reviews_5star,train_percentage)\n",
    "reviews_test = reviews_1star_test + reviews_5star_test\n",
    "y_test  = [1 for i in range(len(reviews_1star_test))] + \\\n",
    "          [5 for i in range(len(reviews_5star_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define two HMMs. `K` is the number of possible hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 8\n",
    "hmm_1 = HMM_TxtGenerator(reviews_1star_train,K)\n",
    "hmm_5 = HMM_TxtGenerator(reviews_5star_train,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the HMMs to their corresponding training sets using expectationâ€“maximization (EM) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `HMM_TxtGenerator.learn_params(num_iter)` repeats EM steps for some iterations, and returns the history of log-likelihood during the steps.\n",
    "At the following, we repeat EM updates for `n_iter` iterations and plot the history of log-likelihood. When log-probability of evidence stops increasing, it means that we can quit EM updates. Please note that log-likelihood might increase dramatically in first iterations. Therefore, in the plot the amount of increase in final iterations would look small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can increase/decrease the number of iterations by setting `n_iter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3 of 50\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-00676f7f3977>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory_loglik_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhmm_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_loglik_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mhistory_loglik_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iteration\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\tum courses\\mlgs23ex1-ge53nag\\generator.py\u001b[0m in \u001b[0;36mlearn_params\u001b[1;34m(self, num_iter)\u001b[0m\n\u001b[0;32m    371\u001b[0m             print(\"iteration \" + str(counter) +\\\n\u001b[0;32m    372\u001b[0m                   \" of \" + str(num_iter) , end=\"\\r\")\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mhistory_loglik\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloglik_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory_loglik\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\tum courses\\mlgs23ex1-ge53nag\\generator.py\u001b[0m in \u001b[0;36mloglik_corpus\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[0mloglik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloglik_of_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwards_backwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m             \u001b[0mloglik\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloglik_of_sent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloglik\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\tum courses\\mlgs23ex1-ge53nag\\generator.py\u001b[0m in \u001b[0;36mforwards_backwards\u001b[1;34m(self, sentence_in)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence_to_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mlog_likelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\tum courses\\mlgs23ex1-ge53nag\\generator.py\u001b[0m in \u001b[0;36mforwards\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m                 \u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iter = 50\n",
    "history_loglik_1 = hmm_1.learn_params(n_iter)\n",
    "plt.figure()\n",
    "plt.plot(range(len(history_loglik_1)) , history_loglik_1)\n",
    "plt.xlabel(\"iteration\",fontsize=16)\n",
    "plt.ylabel(\"log-likelihood\",fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 50\n",
    "history_loglik_5 = hmm_5.learn_params(n_iter)\n",
    "plt.figure()\n",
    "plt.plot(range(len(history_loglik_5)) , history_loglik_5)\n",
    "plt.xlabel(\"iteration\",fontsize=16)\n",
    "plt.ylabel(\"log-likelihood\",fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data generation\n",
    "The trained models can be used to generate new 1- and 5-star reviews of a specified length.  \n",
    "**Before proceeding, implement** the generative process described in \"1. Introduction\" in the function `HMM_TxtGenerator.generate_sentence` in `generator.py` (2pt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 1star review: \n",
      "['trip', 'honor', 'trip', 'an', 'of', 'trip', 'upcoming', 'upcoming', 'upcoming', 'trip', 'an', 'Vegas', 'honor', 'Vegas', 'of']\n",
      "\n",
      "\n",
      "generated 5star review: \n",
      "['turf', 'turf', 'surf', 'of', 'one', 'one', 'The', 'here', 'and', 'was', 'was', 'turf', 'of', 'of', 'was']\n"
     ]
    }
   ],
   "source": [
    "sample_1star = hmm_1.generate_sentence(15)\n",
    "sample_5star = hmm_5.generate_sentence(15)\n",
    "print(\"generated 1star review: \")\n",
    "print(sample_1star)\n",
    "print(\"\\n\")\n",
    "print(\"generated 5star review: \")\n",
    "print(sample_5star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification\n",
    "\n",
    "Now assume that we have a sentence of length $T$ and we want to classify it as either 1-star or 5-star. For this task, we consider the following probabilistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "image/png": {
       "width": 250
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename =\"task_01_classification.png\" , width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model, $Y\\in \\lbrace 1,5 \\rbrace$ indicates the class of the review, either 1-star or 5-star. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative process of the above model is as follows:\n",
    "* Generate the class of the sentence:\n",
    "\\begin{equation}\n",
    "Pr(Y=1) = p \\;\\;\\;\\;, \\;\\;\\;\\; Pr(Y=5) = 1-p \\;\\;.\n",
    "\\end{equation}\n",
    "* If 1-star (resp. 5-star) category is chosen, generate the sentence from marginal distribution of HMM 1 (resp. 5):\n",
    "\\begin{equation}\n",
    "Pr(X_{1:T} | Y) = \n",
    "\\begin{cases}\n",
    "Pr(X_{1:T}|\\pi_1,\\mathbf{A}_1,\\mathbf{B}_1) \\;\\;\\;\\;\\;\\; Y=1 \\\\\n",
    "Pr(X_{1:T}|\\pi_5,\\mathbf{A}_5,\\mathbf{B}_5) \\;\\;\\;\\;\\;\\; Y=5\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify a sentence $X_{1:T}$, we can simply compare $Pr(Y=1|X_{1:T})$ with $Pr(Y=5|X_{1:T})$.\n",
    "\n",
    "Due to Bayes' theorem, this is equivalent to comparing\n",
    "$logPr(Y=1)+logPr(X_{1:T}|\\pi_1,\\mathbf{A}_1,\\mathbf{B}_1)$ and $logPr(Y=5)+ logPr(X_{1:T}|\\pi_5,\\mathbf{A}_5,\\mathbf{B}_5)$. More precisely:\n",
    "* The terms $logPr(X_{1:T}|\\pi_1,\\mathbf{A}_1,\\mathbf{B}_1)$ and $logPr(X_{1:T}|\\pi_5,\\mathbf{A}_5,\\mathbf{B}_5)$ are provided by the functions `hmm_1.loglik_sentence` and `hmm_5.loglik_sentence`. \n",
    "* We can approximate $Pr(Y=1)$ and $Pr(Y=5)$ simply by computing the frequency of each class in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before proceeding, implement** this classification procedure in the function `classify_review` in `classification.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that if a sentence contains a word which is **not** in the vocabulary of a model, the likelihood of the sentence is zero and log-likelihood is $-\\infty$.  \n",
    "In the the following, we drop the samples in test set which have $-\\infty$ log-likelihood in both models. We store all other samples in `reviews_test_filtered` and their categories in `y_test_filtered`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_reviews = []\n",
    "temp_y = []\n",
    "for counter in range(len(reviews_test)):\n",
    "    current_review = reviews_test[counter]\n",
    "    current_y   = y_test[counter]\n",
    "    if(hmm_1.is_in_vocab(current_review) | hmm_5.is_in_vocab(current_review)):\n",
    "        temp_reviews.append(current_review)\n",
    "        temp_y.append(current_y)\n",
    "reviews_test_filtered = temp_reviews\n",
    "y_test_filtered = temp_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate our classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy for 206 test instances: 0.6262135922330098\n"
     ]
    }
   ],
   "source": [
    "p = len(reviews_1star_train)/(len(reviews_1star_train)+len(reviews_5star_train))\n",
    "y_pred = []\n",
    "for sent in reviews_test_filtered:\n",
    "    y_pred.append(classify_review(hmm_1,hmm_5,p,sent))\n",
    "accuracy = np.sum(np.array(y_pred)==np.array(y_test_filtered))/len(y_test_filtered)\n",
    "print(\"classification accuracy for \" + str(len(y_test_filtered)) +\\\n",
    "      \" test instances: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
